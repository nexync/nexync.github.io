---
title: "About Me"
sitemap: false <
permalink: /
---

<link rel="stylesheet" href="../assets/style.css">
<link href="https://cdn.jsdelivr.net/npm/bootstrap-icons/font/bootstrap-icons.css" rel="stylesheet">

I am an incoming PhD student to Princeton Language and Intelligence (PLI) at Princeton University, advised by <a href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a>. Previously, I recieved my Master's from Johns Hopkins University, advised by <a href="https://www.cs.jhu.edu/~vandurme/">Benjamin Van Durme</a>. 

I am broadly interested in language models and exploring their capabilities and limitations. Language models are the foundation of complex systems such as AI agents and multi-agent systems. As such, I believe we should (try our best to) understand the behavior of these models. In particular, I am interested in the below problems:
  - Data: What has a language model seen during training? How does pretraining data influence language models and can we attribute content generated by models back to their pretraining corpus? How can we correct misalignments arising from knowledge conflicts in models' pretraining data?
  - Efficient Language Models: How do we ameliorate the quadratic computational complexity of attention? How much of the key-value cache is really needed for models to maintain performance? Can we make language models more efficient by shifting away from a discrete token space and perform reasoning in continuous latent space?

News
---
 
<table>
	<tr>
		<td width="15%">Feb 2025</td><td>New preprint, Is That Your Final Answer?, released! <a href="https://arxiv.org/abs/2502.13962">[paper]</a> <a href="https://x.com/williamjurayj/status/1892592057073512913">[tweets]</a> </td>
  	</tr>
	<tr>
		<td width="15%">Dec 2024</td><td>New preprint, Compressed Chain of Thought,  released! <a href="https://arxiv.org/abs/2412.13171">[paper]</a> <a href="https://x.com/jeff_cheng_77/status/1869474515325190299">[tweets]</a> </td>
  	</tr>
	<tr>
		<td width="15%">Oct 2024</td><td>Attended CoLM 2024 and presented Dated Data <b>(Outstanding Paper Award, 0.4% <i class="bi bi-trophy"></i>)</b></td>
	</tr>
	<tr>
		<td width="15%">Jul 2024</td><td>Dated Data accepted to CoLM 2024!</td>
	</tr>
	<tr>
		<td width="15%">Mar 2024</td><td>New preprint, Dated Data, is out! <a href="https://arxiv.org/abs/2403.12958">[paper]</a> <a href="https://x.com/jeff_cheng_77/status/1772355368649187669">[tweets]</a> </td>
	</tr>
</table>
